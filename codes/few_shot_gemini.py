# -*- coding: utf-8 -*-
import json
import base64
import re
import os
import time
import csv
import random
import httpx
import pandas as pd
from pathlib import Path
from tqdm import tqdm
from openai import OpenAI
from qcloud_cos import CosConfig, CosS3Client

# ===================== 0. Debug æ§åˆ¶ =====================
PRINT_INVALID_DETAIL = True
PRINT_EXCEPTION_DETAIL = True
PRINT_RAW_PREVIEW_ON_PARSE_FAIL = True
RAW_PREVIEW_CHARS = 600

# ===================== 0. å¢é‡å†™å…¥ï¼ˆæ–°å¢ï¼‰ =====================
WRITE_EVERY_ROW = True       
RESUME_IF_EXISTS = True         

# ===================== 0. é€Ÿç‡é™åˆ¶ï¼ˆæ–°å¢ï¼‰ =====================
SLEEP_PER_ROW_SEC = 1.0       
RATE_LIMIT_MAX_SLEEP = 60.0    
RATE_LIMIT_BASE_SLEEP = 2.0   
RATE_LIMIT_JITTER = 0.3       
RATE_LIMIT_MAX_RETRY = 20    

# ===================== 1. è·¯å¾„ä¸é…ç½® =====================
WORKSPACE_DIR = Path("TEST")
TASK_CSV = "label_title_3005.csv"
TITLE_LOOKUP_CSV = str(WORKSPACE_DIR / "label_title_3005.csv")

RESULT_DIR = WORKSPACE_DIR / "ohmygpt_results"
RESULT_DIR.mkdir(exist_ok=True, parents=True)

# --- OhMyGPT ä¸­è½¬ç«™é…ç½® ---
MODEL_NAME = "gemini-2.0-flash"   
BASE_URL = "https://api.ohmygpt.com/v1"
API_KEY = "sk-xxxx" 
# ===================== 2. ç½‘ç»œé…ç½® =====================
http_client = httpx.Client(
    trust_env=True,
    timeout=httpx.Timeout(connect=30.0, read=180.0, write=30.0, pool=30.0),
)
ai_client = OpenAI(api_key=API_KEY, base_url=BASE_URL, http_client=http_client)

# ===================== 2.1 COS é…ç½® =====================
COS_CONFIG = {
    'SecretId': 'xxxx',
    'SecretKey': 'xxxx',
    'Region': 'xxxx',
    'Bucket': 'xxxx'
}
# ===================== 3. æç¤ºè¯ä¸æ¡ˆä¾‹ =====================
BASE_HOOK_DEFINITIONS = r"""
You are a social media content analyst. Analyze the following post (Title and Cover Image) for psychological hooks. 
For each hook, output 1 if present, 0 if not.

[Definitions]
hook1: Fear Of Missing Out (FOMO)
æ ¸å¿ƒå®šä¹‰ï¼šå†…å®¹ç¤ºæ„äº†â€œå¦‚æœä½ ä¸åšæˆ–é”™è¿‡æŸäº‹ï¼Œå°±ä¼šæœ‰ä»€ä¹ˆæ ·çš„æŸå¤±â€ï¼Œä»¥æ¿€å‘å—ä¼—çš„æ‹…å¿§å’Œå¢é•¿ç´§å¼ æƒ…ç»ªã€‚
æ“ä½œåŒ–åˆ¤æ–­ï¼šå†…å®¹ä¸­æ˜¯å¦åŒ…å«/ä¼ è¾¾äº†ï¼šâ€œä¸è¡ŒåŠ¨â€çš„çº¿ç´¢ AND â€œä¸è¡ŒåŠ¨â€çš„â€œä»£ä»·â€ã€â€œåæœâ€
å‚ç…§çº¿ç´¢ï¼š
- ä¸è¡ŒåŠ¨çš„çº¿ç´¢ï¼šä¸çœ‹/ä¸å¬/ä¸åš/ä¸åƒå¸–å­ä¸­è¿™æ ·çš„è¯...
- â€œä¸è¡ŒåŠ¨â€çš„â€œä»£ä»·â€ã€â€œåæœâ€åŒ…æ‹¬ï¼šç ´äº§/åˆ†æ‰‹/å¤±è´¥...ç­‰è´Ÿé¢æ„å‘
Instruction: Do NOT label '1' just for negative words (sad, bad). In addition to negative words, there also needs to be clues about missing out or inaction.


hook2: Gain Appeal
æ ¸å¿ƒå®šä¹‰ï¼šå†…å®¹å¼ºè°ƒäº†â€œé€šè¿‡æ­¤å†…å®¹ä¿¡æ¯èƒ½è·å¾—ä»€ä¹ˆå¥½å¤„â€ï¼Œä»¥æ¿€å‘å—ä¼—æœ¬èƒ½çš„è·å–åŠ¨æœºã€‚
æ“ä½œåŒ–åˆ¤æ–­ï¼šå†…å®¹ä¸­æ˜¯å¦åŒ…å«/ä¼ è¾¾äº†ï¼šå†…å®¹èƒ½å¸¦æ¥çš„å¥½å¤„
å‚ç…§çº¿ç´¢ï¼š
- å¥½å¤„ï¼šé‡‘é’±ï¼ˆçœé’±ã€èµšé’±ï¼‰ã€æ—¶é—´ï¼ˆèŠ‚çœæ—¶é—´ã€æé«˜æ•ˆç‡ï¼‰ã€å¥åº·ï¼ˆå˜ç˜¦ã€å˜ç¾ï¼‰ã€æŠ€èƒ½ï¼ˆé€Ÿæˆã€ç²¾é€šï¼‰ã€æƒ…æ„Ÿï¼ˆå¿«ä¹ã€å®‰å¿ƒï¼‰â€¦ç­‰æ­£é¢æ„å‘


hook3: Information-gap
æ ¸å¿ƒå®šä¹‰ï¼šå†…å®¹åœ¨æ ‡é¢˜ã€å°é¢æœ¬å¯ä»¥å®Œæ•´è¡¨è¾¾ã€æ¦‚æ‹¬å…¶å†…å®¹ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œå´æ•…æ„æŒ–ç©ºéƒ¨åˆ†ä¿¡æ¯ï¼Œä»¥å¼•å¯¼è§‚ä¼—ç‚¹å¼€å»æ‰¾ã€‚
æ“ä½œåŒ–åˆ¤æ–­ï¼šä½œè€…æ˜¯å¦æœ‰æ„å›¾ï¼šæ•…æ„éšè—ä¿¡æ¯
å‚ç…§çº¿ç´¢ï¼š
- è‡ªé—®è‡ªç­”ï¼ˆå±äºé€šè¿‡é—®é¢˜æ¢é’ˆå½¢å¼çš„ç‰¹æ®Šä¿¡æ¯ç¼ºå£ç±»å‹ï¼‰ï¼šåœ¨æ ‡é¢˜æˆ–å°é¢æå‡ºé—®é¢˜ï¼Œåœ¨ç‚¹å‡»åçš„å†…å®¹ä¸­å›ç­”
- é®æŒ¡å…³é”®ä¿¡æ¯ï¼šç”¨é©¬èµ›å…‹ã€è´´çº¸ç­‰é®æŒ¡æ ‡é¢˜æˆ–å°é¢çš„å…³é”®éƒ¨åˆ†
- è¯åªè¯´ä¸€åŠï¼šç”¨çœç•¥å·ã€ä¸­æ–­ã€ç•™ç™½ç­‰æ–¹å¼æˆªæ–­å¥å­æˆ–æ•…äº‹
- è®¾ç½®æ‚¬å¿µï¼šä½¿ç”¨å„ç§å½¢å¼å¯¹ç¼ºå¤±çš„ä¿¡æ¯è¿›è¡Œé“ºå«ã€æ¸²æŸ“
- åªæŠ›å‡ºæƒ…å¢ƒï¼šä¾‹å¦‚â€œå½“...â€ã€â€œpovï¼šâ€¦â€
- æŒ‡ä»£ä¸æ˜ï¼šç”¨"è¿™ä¸ª""é‚£ä¸ª"æŒ‡ä»£ï¼Œä½†ä¸çŸ¥é“æŒ‡ä»£çš„åˆ°åº•æ˜¯ä»€ä¹ˆ
ï¼ˆæˆ–ï¼šä»¥ä¸Šæ²¡æœ‰åˆ—ä¸¾ä½†ç¬¦åˆæ ¸å¿ƒå®šä¹‰çš„çº¿ç´¢ï¼‰
æ³¨æ„ï¼šAlmost all social media cards have titles. Do NOT label 'Information Gap' just because there is a title.
Core defined boundaries (to avoid cognitive divergence)ï¼š
1.Exclusionary boundaries: Incomplete information caused by the limitations of preview section in terms of length and display format (such as the upper limit of title characters, cover image size) does not belong to information gaps;
2.Initiative boundaries: The gap is subjectively and deliberately designed by the creator, rather than being objectively restricted in content expression. The core is "could have finished but intentionally didn't";
3.Core boundaries: What is missing is the core information of the content (such as results, answers, key details, core conclusions), not insignificant auxiliary information.


hook4: Anomaly and novelty
æ ¸å¿ƒå®šä¹‰ï¼šå†…å®¹è¢«æ•…æ„åŒ…è£…æˆæƒŠäººã€è¿åå¸¸ç†æˆ–ç½•è§ã€æ–°å¥‡çš„ï¼Œä»¥æ¿€å‘å—ä¼—çš„å¥½å¥‡å¿ƒã€‚
æ“ä½œåŒ–åˆ¤æ–­ï¼šå†…å®¹ä¸­æ˜¯å¦åŒ…å«äº†ï¼šè¡¨ç°æƒŠäººåå¸¸çš„çŸ­è¯­ OR è¡¨ç°ç½•è§æ–°å¥‡çš„çŸ­è¯­
å‚ç…§çº¿ç´¢ï¼š
- è¡¨ç°æƒŠäººåå¸¸çš„çŸ­è¯­ï¼šç«Ÿç„¶/å±…ç„¶/æ²¡æƒ³åˆ°/ä¸å¯æ€è®®/ç½•è§/ç¬¬ä¸€æ¬¡è§/éœ‡æƒŠ/æƒŠå‘†/çœ‹å‚»/åˆ·æ–°ä¸‰è§‚/åˆ·æ–°è®¤çŸ¥/ä¸å¯æ€è®®/ç¥å¥‡â€¦
- è¡¨ç°ç½•è§æ–°å¥‡çš„çŸ­è¯­åŒ…æ‹¬ï¼š
  - ç›´æ¥å£°ç§°æ–°å¥‡çš„è¯ï¼šç‹¬åˆ›/åˆ«å…·ä¸€æ ¼/æ ‡æ–°ç«‹å¼‚/æ–°é¢–â€¦
  - æé™è¯ï¼šæœ€/é¡¶/è¶…/ç¬¬ä¸€/å²è¯—çº§â€¦
  - ç¨€ç¼ºæ€§ï¼šå”¯ä¸€/åªæœ‰/é™å®š/é²œè§/å¶å‘/å­¤ä¾‹å­¤å“/å°ä¼—/å†·é—¨/åƒå¹´ä¸€é‡â€¦
ï¼ˆæ³¨æ„ï¼šå¦‚æœå†…å®¹æœ¬èº«å¤Ÿåå¸¸æ–°å¥‡ä½†æ²¡æœ‰è¢«ä½œè€…åŒ…è£…ï¼Œåˆ™ä¸å±äºæ­¤ç±»å‹ï¼›æ­¤å¤–ï¼Œå¸¸è§„åˆ†äº«ã€æƒ…ç»ªå®£æ³„ä¸­åªæœ‰åŒ…å«äº†ä¸Šè¿°è¯æ±‡æˆ–ç±»ä¼¼è¯æ±‡çš„æ‰å±äºæ­¤ç±»å‹ï¼‰
Instruction: Do NOT label '1' just for What you consider novel, interesting, or contrary to common sense. What we are looking for is the action and intention of the author in packaging the content into a striking contrast or rare novelty.


hook5: Perceptual Contrast
æ ¸å¿ƒå®šä¹‰ï¼šå†…å®¹é€šè¿‡è§†è§‰æˆ–è€…æ–‡å­—å°†ä¸¤ç§æˆ–å¤šç§å½¢æˆåå·®æ„ä¹‰çš„çŠ¶æ€æˆ–äº‹ç‰©æ”¾åœ¨ä¸€èµ·ï¼Œä»¥æ¿€å‘å—ä¼—çš„æ¢ç´¢æ¬²ã€‚
æ“ä½œåŒ–åˆ¤æ–­ï¼šæœ‰æ²¡æœ‰ä¸¤ä¸ªåŠå¤šä¸ªæœ‰æ˜æ˜¾åå·®æ„ä¹‰çš„å¯¹æ¯”é¡¹ï¼Œæœ‰æ¯”è¾ƒä½†å·®åˆ«ä¸å¤§çš„ä¸ç®—
å‚ç…§çº¿ç´¢ï¼š
- å¯¹æ¯”é¡¹åŒ…æ‹¬ï¼šå‰å/å·¦å³/æ­£å/å¥½å/é¢„æœŸä¸ç°å®/åˆ«äººä¸è‡ªå·±â€¦ç­‰
- å¯¹æ¯”å½¢å¼åŒ…æ‹¬ï¼šæ–‡æœ¬å’Œæ–‡æœ¬çš„è¯­ä¹‰åå·®ã€å›¾åƒä¹‹é—´çš„è§†è§‰åå·®ã€æ–‡æœ¬ä¸å›¾åƒä¹‹é—´çš„åå·®


hook6: Ingroup Identification / Outgroup Distinction
æ ¸å¿ƒå®šä¹‰ï¼šå†…å®¹é€šè¿‡ç¾¤ä½“æ ‡ç­¾ï¼Œæ¿€å‘æŸä¸€ç¾¤ä½“å†…çš„è®¤åŒã€å½’å±ï¼›æˆ–æ¿€å‘å¯¹æŸä¸€ç¾¤ä½“çš„æ’æ–¥ã€è°ƒä¾ƒã€‚
æ“ä½œåŒ–åˆ¤æ–­ï¼šå†…å®¹ä¸­æ˜¯å¦ï¼šå‡ºç°ç¾¤ä½“æ ‡ç­¾ ANDï¼ˆå‡ºç°å½’å±/æ’æ–¥æ€åº¦ OR è¡ŒåŠ¨å¬å”¤ OR ç¾¤ä½“å…±æ€§ï¼‰
å‚ç…§çº¿ç´¢ï¼š
- ç¾¤ä½“æ ‡ç­¾ï¼šæåŠæŸä¸€ç¾¤ä½“ï¼Œå¦‚
  - æåˆ°ï¼šå’Œç§æ—/å›½å®¶/æ°‘æ—/å®—æ•™/åœ°åŸŸâ€¦ç­‰ç›¸å…³çš„åè¯
  - æåˆ°ï¼šå’Œå­¦æ ¡/å…¬å¸/ç»„ç»‡/æœºæ„/ç¤¾åŒºâ€¦ç­‰ç›¸å…³çš„åè¯
  - æåˆ°ï¼šå’Œå¹´é¾„/æ€§åˆ«/èŒä¸šâ€¦ç­‰ç›¸å…³çš„åè¯
  - ä»¥åŠå…¶ä»–ï¼šæŸç§çˆ±å¥½æ ‡ç­¾/æŸç§æ€§æ ¼æ ‡ç­¾/æŸç§æ˜Ÿåº§æ ‡ç­¾/æŸç§mbtiæ ‡ç­¾â€¦ç­‰
  ï¼ˆåªè¦å‡ºç°çš„çŸ­è¯­å¯ä»¥åœ¨äººç¾¤ä¸­åˆ’åˆ†å‡ºä¸€ä¸ªç¾¤ä½“å’Œå¦å¤–çš„äººï¼Œå³å¯ï¼‰
- å½’å±/æ’æ–¥æ€åº¦ï¼šè¡¨ç°å‡ºéª„å‚²ã€è‡ªè±ªã€è®¤åŒã€å…±æƒ…â€¦ï¼›æˆ–ï¼šé„™è§†ã€è°ƒä¾ƒã€è®½åˆºâ€¦ç­‰
- è¡ŒåŠ¨å¬å”¤ï¼šâ€œ...å¿…çœ‹â€ã€â€œæ˜¯...å°±ç‚¹èµâ€ã€â€œ...ä»¬è¡ŒåŠ¨èµ·æ¥â€â€¦ç­‰
- ç¾¤ä½“å…±æ€§ï¼šâ€œæ¯ä¸ª...éƒ½ç»å†è¿‡â€ã€â€œ...çš„æ—¥å¸¸â€ã€â€œ...éƒ½æ‡‚â€â€¦ä»¥åŠä½“ç°åœ¨ç”»é¢ä¸­çš„å…±æ€§ç‰¹å¾ç­‰
Instruction: Be SENSITIVE. If you suspect the content contains any jargon or visual style specific to a niche group, even if you are not 100% sure, Please also take it into consideration.


hook7: Social Comparison
æ ¸å¿ƒå®šä¹‰ï¼šÂ å†…å®¹é€šè¿‡ç›´æ¥ä½¿ç”¨æ˜æ˜¾çš„æ¯”è¾ƒè¯ã€å±•ç¤ºå·®è·ã€æˆ–å±•ç¤ºç¤¾ä¼šæ¯”è¾ƒåçš„æŸç§æ€åº¦ç­‰ï¼Œæ¥å¼•å‘å—ä¼—å‚ä¸æ¯”è¾ƒã€‚
æ“ä½œåŒ–åˆ¤æ–­ï¼š
å†…å®¹ä¸­æ˜¯å¦ï¼šæ˜æ˜¾çš„æ¯”è¾ƒè¡Œä¸ºè¯ ORï¼ˆå±•ç¤ºå·®è· AND å±•ç¤ºäº†ç¤¾ä¼šæ¯”è¾ƒåæ€åº¦ï¼‰
å‚ç…§çº¿ç´¢ï¼š
- æ˜æ˜¾çš„æ¯”è¾ƒè¡Œä¸ºè¯ï¼šæ¯”.../æ›´/VS/ä¸å¦‚.../ç¢¾å‹/ç§’æ€â€¦ç­‰
- å·®è·ï¼šèƒ½åŠ›å·®è·ï¼ˆæŠ€èƒ½ã€æˆå°±ã€ä»»åŠ¡ç»©æ•ˆç­‰ï¼‰/ä¸ªäººç‰¹è´¨å·®è·ï¼ˆå¤–è²Œã€æ€§æ ¼ã€å¤©èµ‹ã€èº«é«˜ç­‰ï¼‰/èµ„æºå·®è·ï¼ˆè´¢å¯Œã€ç”Ÿæ´»æ°´å¹³ã€åœ°ä½é«˜ä½ã€æƒåˆ©å·®åˆ«ç­‰ï¼‰â€¦
- æ¯”è¾ƒåçš„æ€åº¦ï¼š -å‘ä¸Šæ¯”è¾ƒï¼šå«‰å¦’ã€è‡ªå‘ç­‰æ¶ˆææ€åº¦ï¼›è®¤å¯ã€æ¿€åŠ±ä¸Šè¿›ç­‰ç§¯ææ€åº¦
                                  -å‘ä¸‹æ¯”è¾ƒï¼šç‚«è€€ã€ä¼˜è¶Šæ„Ÿç­‰æ¶ˆææ€åº¦ï¼›çæƒœã€çŸ¥è¶³ç­‰ç§¯ææ€åº¦
æ³¨æ„ï¼š
1.å·®è·çš„å‚ç…§ç‚¹å¯ä»¥æ˜¯å†…å®¹ä¸­ç›´æ¥å±•ç°çš„å¯¹è±¡ï¼Œä¹Ÿå¯ä»¥æ˜¯å¤§ä¼—é»˜è®¤çš„æ™®éæ°´å¹³ï¼›å› æ­¤è¯·æ’é™¤å¸¸è§„çš„æ™®éæ°´å¹³çš„åˆ†äº«ç±»ç¬”è®°ï¼Œè¿™ä¸æ„æˆç¤¾ä¼šæ¯”è¾ƒã€‚
2.ç¤¾ä¼šæ¯”è¾ƒçš„æ¯”è¾ƒå¯¹è±¡æ˜¯è‡ªå·±ä¸åˆ«äººæ¯”è¾ƒã€åˆ«äººä¸åˆ«äººæ¯”è¾ƒã€ç¾¤ä½“ä¸ç¾¤ä½“æ¯”è¾ƒï¼ˆä¸åŒ…å«è‡ªå·±ä¸è‡ªå·±æ¯”è¾ƒã€å•çº¯çš„ç‰©å“æ¯”è¾ƒï¼‰


hook8: Authority Endorsement
æ ¸å¿ƒå®šä¹‰ï¼šæ–‡å­—æˆ–å›¾ç‰‡é€šè¿‡å„ç§æœ‰è¯´æœåŠ›çš„ä¿¡æºèƒŒä¹¦æ¥è±å…å—ä¼—çš„è´¨ç–‘æˆæœ¬ï¼Œä»è€Œå¼•å¯¼å—ä¼—ä¿¡æœæˆ–æ¨¡ä»¿ã€‚
æ“ä½œåŒ–åˆ¤æ–­ï¼šæ–‡å­—æˆ–å›¾ç‰‡ä¸­æ˜¯å¦å‡ºç°ï¼šä¿¡æºèƒŒä¹¦
å‚ç…§çº¿ç´¢ï¼š
- æƒå¨ä¿¡æºåŒ…æ‹¬ï¼šä¸“å®¶/æ•™æˆ/æœºæ„/åäºº/ç ”ç©¶/æ’å/è®¤è¯/å¥–é¡¹/æ•°å­—â€¦ç­‰
  - å¦‚ï¼šå“ˆä½›å¤§å­¦ç ”ç©¶/æ®...æŠ¥é“/...ä¸“å®¶è¯´/FDAè®¤è¯/æŸæŸæ˜æ˜ŸåŒæ¬¾/é’å²›ç¬¬ä¸€çš„/22wäººçœ‹è¿‡çš„â€¦
æ³¨æ„ï¼šå‡¡æ˜¯èƒ½å¢åŠ å¯ä¿¡åº¦çš„çº¿ç´¢éƒ½åœ¨èŒƒå›´å†…ï¼Œå…ˆæ ‡æ³¨èµ·æ¥æ¯”æ¼æ ‡è¦å¥½ã€‚

[Note] 
1. We are in the discovery phase. If a post shows even a slight tendency or subtle hint of a hook, please lean towards labeling it as 1.
2. First, find reasons to label it as 1; only if no clues exist, label it as 0.
3. INDEPENDENT JUDGMENT: The [Reference Examples] provided below are for reference and reasoning logic understanding ONLY. Judge the target post independently based on its own content.
4. Definitionsæ˜¯æœ€é‡è¦çš„ï¼Œè®°å¾—ä»¥çœ‹å®šä¹‰ä¸ºä¸»ï¼Œå‚è€ƒæ ·æœ¬ä»…è¾…åŠ©ä½œç”¨

[Json Output Format]
Return ONLY ONE valid JSON object (no markdown, no ```).
Keys MUST appear in this exact order. All h1~h8 MUST be 0 or 1.
"reasoning" MUST be Chinese and <= 30 characters. "reasoning" may be empty.

{
  "h1":0,
  "h2":0,
  "h3":0,
  "h4":0,
  "h5":0,
  "h6":0,
  "h7":0,
  "h8":0,
  "reasoning":"<=30å­—ä¸­æ–‡åŸå› (å¯ä¸ºç©º)"
}
"""

HOOK_CASES = {
    1: [
        ("64c4d22f0000000017019365", "Inaction (ignoring diet) leads to the cost of wasted workouts, triggering FOMO.", 1),
        ("69706f2e0000000022030f10", "Failing to choose the right fridge causes health risks, fitting FOMO's inaction-cost logic.", 1),
        ("63f8c1d20000000012032891", "No inaction-cost link; just personal avoidance, not FOMO-inducing loss fear.", 0),
    ],
    2: [
        ("68f0b6ee000000000400330b", "Offers styling skills to look better and avoid a plain appearance", 1),
        ("6960f3ea00000000210284d0", "Provides curated autumn routes to save time and enjoy fall views efficiently", 1),
        ("640d649400000000130087a1", "Only shows bedroom design with no stated benefits for the audience", 0),
    ],
    3: [
        ("696f532b000000001a037aee", "Truncates with ellipsis after 'unless', intentionally hiding key conditions to drive clicks", 1),
        ("690ca1f5000000000700e328", "Withholds the suspense dramaâ€™s name, creating a core info gap to lure clicks", 1),
        ("6440c966000000001300a8cc", "No deliberate hiding; title fully conveys core Meta layoff news clearly", 0),
    ],
    4: [
        ("63fc10dd0000000013010d59", "Author uses 'å“ä¸€è·³' to frame Â¥100 skewers as astonishingly abnormal.", 1),
        ("68ecbd86000000000303b778", "Author uses 'å°ç¥çº§åˆ«' to package Hokkaido winter as extremely rare/novel.", 1),
        ("644a7b200000000013002c23", "No astonishing/rare packaging; normal sharing about cute bags.", 0),
    ],
    5: [
        ("64ad5371000000001c00dac1", "Shows stark visual contrast between day1 and day3 belly states.", 1),
        ("64565f230000000011010f20", "Contrasts DIY cost ($200+) with claimed $1799 value sharply.", 1),
        ("6969de13000000002102ab83", "Lipstick shades differ slightly; no significant contrast.", 0),
    ],
    6: [
        ("696c839b0000000022021da9", "Mentions 'Chinese women(ä¸­å¥³)' as group label + ingroup belonging/affirmation.", 1),
        ("6965bc51000000000d009ec1", "Uses 'Li Zeyan's fans' group label + calls for group attention/action.", 1),
        ("6445065300000000120334a7", "Lacks a clear group label or ingroup/outgroup attitude.", 0),
    ],
    7: [
        ("690c6efa000000000400119d", "Uses 'not inferior to' to compare targets in status/achievement.", 1),
        ("6470075400000000270118bc", "Shows gap in figure/appearance beyond average + attitude to improve.", 1),
        ("641c0cd5000000001303ee42", "Only states loneliness as normal; no comparison/gap/attitude.", 0),
    ],
    8: [
        ("6429995c0000000013030a41", "CCTV endorsement acts as authority to reduce audience doubt.", 1),
        ("643ba7940000000012032cb1", "TV station feature serves as authority endorsement for credibility.", 1),
        ("69677b67000000001a02e15e", "No real authoritative source; only subjective 'authority' claim.", 0),
    ],
}

# ===================== 4. å·¥å…·å‡½æ•° =====================
def get_cos_client():
    if not COS_CONFIG["SecretId"] or not COS_CONFIG["SecretKey"]:
        raise RuntimeError("è¯·è®¾ç½® COS_SECRET_ID / COS_SECRET_KEY ç¯å¢ƒå˜é‡ï¼ˆæˆ–å†™æ­»åœ¨ COS_CONFIG é‡Œï¼‰")
    config = CosConfig(
        Region=COS_CONFIG["Region"],
        SecretId=COS_CONFIG["SecretId"],
        SecretKey=COS_CONFIG["SecretKey"]
    )
    return CosS3Client(config)

def load_title_map(label_title_csv: str) -> dict:
    df = pd.read_csv(label_title_csv, dtype=str, keep_default_na=False)
    df["post_id"] = df["post_id"].astype(str).str.strip().str.lower()
    df["title"] = df["title"].astype(str)
    return dict(zip(df["post_id"], df["title"]))

def build_cases_block(title_map: dict) -> str:
    lines = ["\n\n[Calibration Cases]\n", "Each hook below has 3 cases. The 'label' refers ONLY to that hook.\n"]
    for hook_id in range(1, 9):
        lines.append(f"\n## hook{hook_id} cases\n")
        for j, (pid, reasoning, label) in enumerate(HOOK_CASES[hook_id], start=1):
            pid_l = pid.strip().lower()
            title = title_map.get(pid_l, "")
            lines.append(f"- case{j}: post_id={pid_l}\n  title={title}\n  reasoning={reasoning}\n  label={label}\n")
    return "\n".join(lines)

def detect_image_mime(data: bytes) -> str:
    # JPEG
    if len(data) >= 3 and data[0:3] == b"\xff\xd8\xff":
        return "image/jpeg"
    # PNG
    if len(data) >= 8 and data[0:8] == b"\x89PNG\r\n\x1a\n":
        return "image/png"
    # WEBP (RIFF....WEBP)
    if len(data) >= 12 and data[0:4] == b"RIFF" and data[8:12] == b"WEBP":
        return "image/webp"
    # GIF
    if len(data) >= 6 and data[0:6] in (b"GIF87a", b"GIF89a"):
        return "image/gif"
    return "image/jpeg"

def fetch_cover_b64_and_mime(cos_client, pid: str):
    key = f"downloads/{pid}/cover.jpg"
    obj = cos_client.get_object(Bucket=COS_CONFIG["Bucket"], Key=key)
    data = obj["Body"].get_raw_stream().read()
    mime = detect_image_mime(data)
    b64 = base64.b64encode(data).decode("utf-8")
    return b64, mime

def safe_parse_json(text: str):
    if not text:
        return None
    s = text.strip()

    s = re.sub(r"```json\s*", "", s, flags=re.IGNORECASE)
    s = s.replace("```", "")

    # ä»ç¬¬ä¸€ä¸ª { å¼€å§‹åšæ‹¬å·é…å¯¹ï¼Œæˆªå–ç¬¬ä¸€ä¸ªå®Œæ•´ JSON å¯¹è±¡
    start = s.find("{")
    if start == -1:
        return None

    depth = 0
    in_str = False
    esc = False
    for i in range(start, len(s)):
        ch = s[i]
        if in_str:
            if esc:
                esc = False
            elif ch == "\\":
                esc = True
            elif ch == '"':
                in_str = False
        else:
            if ch == '"':
                in_str = True
            elif ch == "{":
                depth += 1
            elif ch == "}":
                depth -= 1
                if depth == 0:
                    candidate = s[start:i+1]
                    try:
                        return json.loads(candidate)
                    except:
                        return None
    return None

def normalize_01(v):
    if isinstance(v, bool):
        return int(v)
    if isinstance(v, (int, float)):
        return int(v) if v in (0, 1) else None
    if isinstance(v, str):
        t = v.strip()
        if t == "0":
            return 0
        if t == "1":
            return 1
    return None

def validate_and_extract(raw_text: str):
    obj = safe_parse_json(raw_text)
    if not isinstance(obj, dict):
        return False, None, "JSON_PARSE_FAILED"

    # é¢å¤–æœºåˆ¶é”®æ£€æŸ¥ï¼šåªè¦å‡ºç° h9/h10 è¿™ç§ï¼Œç›´æ¥åˆ¤ invalid
    for k in obj.keys():
        m = re.match(r"^(h|hook)(\d+)$", str(k).strip())
        if m and (int(m.group(2)) < 1 or int(m.group(2)) > 8):
            return False, None, f"EXTRA_HOOK_KEY_{k}"

    extracted = {}

    # h1~h8 å¿…é¡»å¯è¯»ä¸º 0/1
    for i in range(1, 9):
        val = normalize_01(obj.get(f"h{i}"))
        if val is None:
            return False, None, f"INVALID_VALUE_h{i}"
        extracted[f"h{i}"] = val

    # reasoning å…è®¸ä¸ºç©ºï¼šç¼ºå¤±/ç©ºä¸²éƒ½ OKï¼Œæœ€åå…œåº•æˆªæ–­ 20 å­—
    r = str(obj.get("reasoning", "") if obj.get("reasoning", "") is not None else "").strip()
    extracted["reasoning"] = r[:20]

    return True, extracted, ""

def _short(s: str, n: int):
    s = "" if s is None else str(s)
    s = s.replace("\n", "\\n")
    return s[:n] + ("..." if len(s) > n else "")

def looks_truncated_json(s: str) -> bool:
    if not s:
        return True
    s = s.strip()
    if "{" in s and "}" not in s:
        return True

    start = s.find("{")
    if start == -1:
        return False

    depth = 0
    in_str = False
    esc = False
    for ch in s[start:]:
        if in_str:
            if esc:
                esc = False
            elif ch == "\\":
                esc = True
            elif ch == '"':
                in_str = False
        else:
            if ch == '"':
                in_str = True
            elif ch == "{":
                depth += 1
            elif ch == "}":
                depth -= 1
                if depth == 0:
                    return False
    return True

def is_rate_limited_exception(e: Exception) -> bool:
    msg = str(e).lower()
    keywords = [
        "rate limit", "ratelimit", "too many requests", "429",
        "quota", "throttl", "exceeded", "temporarily unavailable"
    ]
    if any(k in msg for k in keywords):
        return True
    if hasattr(e, "status_code") and getattr(e, "status_code", None) == 429:
        return True
    if hasattr(e, "response") and getattr(e, "response", None) is not None:
        try:
            if getattr(e.response, "status_code", None) == 429:
                return True
        except Exception:
            pass
    return False

def format_exception_detail(e: Exception) -> str:
    parts = [f"{type(e).__name__}: {e}"]
    if hasattr(e, "status_code") and getattr(e, "status_code", None):
        parts.append(f"status_code={getattr(e, 'status_code')}")
    if hasattr(e, "body") and getattr(e, "body", None):
        parts.append(f"body={_short(getattr(e, 'body'), 800)}")
    if hasattr(e, "response") and getattr(e, "response", None):
        try:
            r = getattr(e, "response")
            if hasattr(r, "status_code"):
                parts.append(f"resp_status={r.status_code}")
        except Exception:
            pass
    return " | ".join(parts)

# ===================== 4.1 å¢é‡å†™å…¥ CSV =====================
def load_done_post_ids(output_csv: Path) -> set:
    if (not output_csv.exists()) or output_csv.stat().st_size == 0:
        return set()
    try:
        df = pd.read_csv(output_csv, dtype=str, usecols=["post_id"], keep_default_na=False)
        return set(df["post_id"].astype(str).str.strip().str.lower().tolist())
    except Exception:
        return set()

def open_csv_appender(output_csv: Path, fieldnames: list):
    need_header = (not output_csv.exists()) or output_csv.stat().st_size == 0
    f = open(output_csv, "a", newline="", encoding="utf-8-sig")
    writer = csv.DictWriter(f, fieldnames=fieldnames)
    if need_header:
        writer.writeheader()
        f.flush()
    return f, writer

def write_one_row(writer, f_handle, rec: dict):
    writer.writerow(rec)
    f_handle.flush()

# ===================== 5. ä¸»æµç¨‹ =====================
def run_test():
    title_map = load_title_map(TITLE_LOOKUP_CSV)
    cases_block = build_cases_block(title_map)
    system_prompt = BASE_HOOK_DEFINITIONS + cases_block

    df_task = pd.read_csv(TASK_CSV, dtype=str, keep_default_na=False)
    df_task["post_id"] = df_task["post_id"].astype(str).str.strip().str.lower()

    cos_client = get_cos_client()

    OUT_COLS = ["post_id"] + [f"h{i}" for i in range(1, 9)] + ["title", "reasoning"]
    output_path = RESULT_DIR / f"predictions_{MODEL_NAME.replace('/', '_')}_incremental.csv"

    done = set()
    if RESUME_IF_EXISTS:
        done = load_done_post_ids(output_path)
        if done:
            print(f"ğŸ” RESUME: æ£€æµ‹åˆ°å·²å®Œæˆ {len(done)} æ¡ï¼Œå°†è‡ªåŠ¨è·³è¿‡ã€‚è¾“å‡ºï¼š{output_path}")

    f_out, writer = open_csv_appender(output_path, OUT_COLS)

    total_model_calls = 0
    invalid_model_calls = 0

    try:
        for _, row in tqdm(df_task.iterrows(), total=len(df_task), desc=f"Running {MODEL_NAME}"):
            pid = row["post_id"]
            if RESUME_IF_EXISTS and pid in done:
                continue

            title = title_map.get(pid, "")

            try:
                img_b64, img_mime = fetch_cover_b64_and_mime(cos_client, pid)
            except Exception as e:
                if PRINT_EXCEPTION_DETAIL:
                    tqdm.write(f"[COS_ERROR] pid={pid} | {format_exception_detail(e)}")
                rec = {
                    "post_id": pid,
                    "title": title,
                    "reasoning": "MISSING_COVER_OR_COS_ERROR",
                    **{f"h{i}": 0 for i in range(1, 9)}
                }
                if WRITE_EVERY_ROW:
                    write_one_row(writer, f_out, rec)
                done.add(pid)
                if SLEEP_PER_ROW_SEC > 0:
                    time.sleep(SLEEP_PER_ROW_SEC)
                continue

            final = None
            last_err = ""

            invalid_attempts = 0  
            rate_retry = 0       
            backoff = RATE_LIMIT_BASE_SLEEP

            while invalid_attempts < 3:
                total_model_calls += 1
                try:
                    resp = ai_client.chat.completions.create(
                        model=MODEL_NAME,
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": [
                                {"type": "text", "text": f"å¾…æ ‡æ³¨æ ‡é¢˜: {title}"},
                                {"type": "image_url",
                                 "image_url": {"url": f"data:{img_mime};base64,{img_b64}"}}
                            ]}
                        ],
                        temperature=0,
                        max_tokens=200, 
                    )

                    raw_content = resp.choices[0].message.content
                    ok, extracted, err = validate_and_extract(raw_content)

                    if ok:
                        final = extracted
                        break

                    if err == "JSON_PARSE_FAILED" and looks_truncated_json(raw_content):
                        rate_retry += 1
                        if PRINT_INVALID_DETAIL:
                            tqdm.write(
                                f"[TRUNCATED_JSON] pid={pid} | retry={rate_retry}/{RATE_LIMIT_MAX_RETRY} "
                                f"| len={len(raw_content)} | tail={repr(raw_content[-80:])}"
                            )
                        if PRINT_RAW_PREVIEW_ON_PARSE_FAIL:
                            tqdm.write(f"[RAW_PREVIEW] pid={pid} | {_short(raw_content, RAW_PREVIEW_CHARS)}")

                        if rate_retry > RATE_LIMIT_MAX_RETRY:
                            last_err = "TRUNCATED_JSON_MAX_RETRY_EXCEEDED"
                            break

                        sleep_s = min(RATE_LIMIT_MAX_SLEEP, backoff)
                        jitter = 1.0 + random.uniform(-RATE_LIMIT_JITTER, RATE_LIMIT_JITTER)
                        sleep_s = max(0.5, sleep_s * jitter)
                        tqdm.write(f"[TRUNCATED_SLEEP] pid={pid} | sleep={sleep_s:.1f}s")
                        time.sleep(sleep_s)
                        backoff *= 2.0
                        continue

                    invalid_model_calls += 1
                    invalid_attempts += 1
                    last_err = err

                    if PRINT_INVALID_DETAIL:
                        tqdm.write(f"[INVALID_OUTPUT] pid={pid} | invalid_try={invalid_attempts}/3 | err={err}")
                        if err == "JSON_PARSE_FAILED" and PRINT_RAW_PREVIEW_ON_PARSE_FAIL:
                            tqdm.write(f"[RAW_PREVIEW] pid={pid} | {_short(raw_content, RAW_PREVIEW_CHARS)}")

                    time.sleep(0.3)

                except Exception as e:
                    if is_rate_limited_exception(e):
                        rate_retry += 1
                        if PRINT_EXCEPTION_DETAIL:
                            tqdm.write(
                                f"[RATE_LIMIT] pid={pid} | retry={rate_retry}/{RATE_LIMIT_MAX_RETRY} | "
                                f"{format_exception_detail(e)}"
                            )
                        if rate_retry > RATE_LIMIT_MAX_RETRY:
                            last_err = "RATE_LIMIT_MAX_RETRY_EXCEEDED"
                            break

                        sleep_s = min(RATE_LIMIT_MAX_SLEEP, backoff)
                        jitter = 1.0 + random.uniform(-RATE_LIMIT_JITTER, RATE_LIMIT_JITTER)
                        sleep_s = max(0.5, sleep_s * jitter)

                        tqdm.write(f"[RATE_LIMIT_SLEEP] pid={pid} | sleep={sleep_s:.1f}s")
                        time.sleep(sleep_s)
                        backoff *= 2.0
                        continue

                    invalid_model_calls += 1
                    invalid_attempts += 1
                    last_err = f"API_EXCEPTION_{type(e).__name__}"
                    if PRINT_EXCEPTION_DETAIL:
                        tqdm.write(f"[API_EXCEPTION] pid={pid} | invalid_try={invalid_attempts}/3 | {format_exception_detail(e)}")
                    time.sleep(0.8)

            if final is None:
                tqdm.write(f"[FINAL_FAIL] pid={pid} | last_err={last_err}")
                rec = {
                    "post_id": pid,
                    "title": title,
                    "reasoning": f"FAILED:{last_err}"[:20],  
                    **{f"h{i}": 0 for i in range(1, 9)}
                }
            else:
                r = (final.get("reasoning", "") or "").strip()[:20]  
                rec = {
                    "post_id": pid,
                    "title": title,
                    "reasoning": r,
                    **{f"h{i}": final[f"h{i}"] for i in range(1, 9)}
                }

            if WRITE_EVERY_ROW:
                write_one_row(writer, f_out, rec)

            done.add(pid)

            if SLEEP_PER_ROW_SEC > 0:
                time.sleep(SLEEP_PER_ROW_SEC)

    finally:
        f_out.close()

    invalid_rate = (invalid_model_calls / total_model_calls) if total_model_calls > 0 else 0.0
    stats = {
        "model_name": MODEL_NAME,
        "task_csv": TASK_CSV,
        "output_csv": str(output_path),
        "total_model_calls": total_model_calls,
        "invalid_model_calls": invalid_model_calls,
        "invalid_rate": f"{invalid_rate:.4%}",
        "processed_rows": len(done),
        "sleep_per_row_sec": SLEEP_PER_ROW_SEC,
        "rate_limit_base_sleep": RATE_LIMIT_BASE_SLEEP,
        "rate_limit_max_sleep": RATE_LIMIT_MAX_SLEEP,
        "rate_limit_max_retry": RATE_LIMIT_MAX_RETRY,
    }
    with open(RESULT_DIR / "stats.json", "w", encoding="utf-8") as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… å®Œæˆï¼è¾“å‡ºï¼š{output_path}")
    print(f"ğŸ“Š è°ƒç”¨æ€»æ•° {total_model_calls} | æ— æ•ˆè¾“å‡º {invalid_model_calls} | æ— æ•ˆç‡ {invalid_rate:.4%} | å·²å†™å…¥ {len(done)} è¡Œ")

if __name__ == "__main__":
    run_test()
