# -*- coding: utf-8 -*-
import json
import base64
import re
import os
import time
import csv
import random
import httpx
import pandas as pd
from pathlib import Path
from tqdm import tqdm
from openai import OpenAI
from qcloud_cos import CosConfig, CosS3Client


# ===================== 0. Debug æ§åˆ¶ =====================
PRINT_INVALID_DETAIL = True
PRINT_EXCEPTION_DETAIL = True
PRINT_RAW_PREVIEW_ON_PARSE_FAIL = True
RAW_PREVIEW_CHARS = 600

# ===================== 0. å¢é‡å†™å…¥ï¼ˆæ–°å¢ï¼‰ =====================
WRITE_EVERY_ROW = True          
RESUME_IF_EXISTS = True        
# ===================== 0. é€Ÿç‡é™åˆ¶ï¼ˆæ–°å¢ï¼‰ =====================
SLEEP_PER_ROW_SEC = 0.0        
RATE_LIMIT_MAX_SLEEP = 60.0     
RATE_LIMIT_BASE_SLEEP = 2.0     
RATE_LIMIT_JITTER = 0.3        
RATE_LIMIT_MAX_RETRY = 20   
# ===================== 1. è·¯å¾„ä¸é…ç½® =====================
WORKSPACE_DIR = Path(r"/data/shencanyu")
TASK_CSV = r"/data/shencanyu/test/part_1.csv"
TITLE_LOOKUP_CSV = str(WORKSPACE_DIR / "label_title.csv")

RESULT_DIR = WORKSPACE_DIR / "qwen2.5-vl-32b-instruct-zero"
RESULT_DIR.mkdir(exist_ok=True, parents=True)

# --- é˜¿é‡Œäº‘ DashScope å…¼å®¹æ¨¡å¼é…ç½® ---
MODEL_NAME = "qwen2.5-vl-32b-instruct"
BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"
API_KEY = "sk-xxxx" 

# ===================== 2. ç½‘ç»œé…ç½® =====================
os.environ["NO_PROXY"] = "dashscope.aliyuncs.com"
http_client = httpx.Client(trust_env=False, proxy=None, timeout=30.0)
ai_client = OpenAI(api_key=API_KEY, base_url=BASE_URL, http_client=http_client)

COS_CONFIG = {
    'SecretId': 'xxxx',
    'SecretKey': 'xxxx',
    'Region': 'xxxx',
    'Bucket': 'xxxx'
}

# ===================== 3. æç¤ºè¯ï¼ˆZero-shotï¼šä¸å†æ‹¼æ¥ hook_casesï¼‰ =====================
BASE_HOOK_DEFINITIONS =  r"""

You are a social media content analyst. Analyze the following post (Title and Cover Image) for psychological hooks. 
For each hook, output 1 if present, 0 if not.

[Definitions]
hook1: Fear Of Missing Out (FOMO)
æ ¸å¿ƒå®šä¹‰ï¼šå†…å®¹ç¤ºæ„äº†â€œå¦‚æœä½ ä¸åšæˆ–é”™è¿‡æŸäº‹ï¼Œå°±ä¼šæœ‰ä»€ä¹ˆæ ·çš„æŸå¤±â€ï¼Œä»¥æ¿€å‘å—ä¼—çš„æ‹…å¿§å’Œå¢é•¿ç´§å¼ æƒ…ç»ªã€‚
æ“ä½œåŒ–åˆ¤æ–­ï¼šå†…å®¹ä¸­æ˜¯å¦åŒ…å«/ä¼ è¾¾äº†ï¼šâ€œä¸è¡ŒåŠ¨â€çš„çº¿ç´¢ AND â€œä¸è¡ŒåŠ¨â€çš„â€œä»£ä»·â€ã€â€œåæœâ€
å‚ç…§çº¿ç´¢ï¼š
- ä¸è¡ŒåŠ¨çš„çº¿ç´¢ï¼šä¸çœ‹/ä¸å¬/ä¸åš/ä¸åƒå¸–å­ä¸­è¿™æ ·çš„è¯...
- â€œä¸è¡ŒåŠ¨â€çš„â€œä»£ä»·â€ã€â€œåæœâ€åŒ…æ‹¬ï¼šç ´äº§/åˆ†æ‰‹/å¤±è´¥...ç­‰è´Ÿé¢æ„å‘
Instruction: Do NOT label '1' just for negative words (sad, bad). In addition to negative words, there also needs to be clues about missing out or inaction.


hook2: Gain Appeal
æ ¸å¿ƒå®šä¹‰ï¼šå†…å®¹å¼ºè°ƒäº†â€œé€šè¿‡æ­¤å†…å®¹ä¿¡æ¯èƒ½è·å¾—ä»€ä¹ˆå¥½å¤„â€ï¼Œä»¥æ¿€å‘å—ä¼—æœ¬èƒ½çš„è·å–åŠ¨æœºã€‚
æ“ä½œåŒ–åˆ¤æ–­ï¼šå†…å®¹ä¸­æ˜¯å¦åŒ…å«/ä¼ è¾¾äº†ï¼šå†…å®¹èƒ½å¸¦æ¥çš„å¥½å¤„
å‚ç…§çº¿ç´¢ï¼š
- å¥½å¤„ï¼šé‡‘é’±ï¼ˆçœé’±ã€èµšé’±ï¼‰ã€æ—¶é—´ï¼ˆèŠ‚çœæ—¶é—´ã€æé«˜æ•ˆç‡ï¼‰ã€å¥åº·ï¼ˆå˜ç˜¦ã€å˜ç¾ï¼‰ã€æŠ€èƒ½ï¼ˆé€Ÿæˆã€ç²¾é€šï¼‰ã€æƒ…æ„Ÿï¼ˆå¿«ä¹ã€å®‰å¿ƒï¼‰â€¦ç­‰æ­£é¢æ„å‘


hook3: Information-gap
æ ¸å¿ƒå®šä¹‰ï¼šå†…å®¹åœ¨æ ‡é¢˜ã€å°é¢æœ¬å¯ä»¥å®Œæ•´è¡¨è¾¾ã€æ¦‚æ‹¬å…¶å†…å®¹ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œå´æ•…æ„æŒ–ç©ºéƒ¨åˆ†ä¿¡æ¯ï¼Œä»¥å¼•å¯¼è§‚ä¼—ç‚¹å¼€å»æ‰¾ã€‚
æ“ä½œåŒ–åˆ¤æ–­ï¼šä½œè€…æ˜¯å¦æœ‰æ„å›¾ï¼šæ•…æ„éšè—ä¿¡æ¯
å‚ç…§çº¿ç´¢ï¼š
- è‡ªé—®è‡ªç­”ï¼ˆå±äºé€šè¿‡é—®é¢˜æ¢é’ˆå½¢å¼çš„ç‰¹æ®Šä¿¡æ¯ç¼ºå£ç±»å‹ï¼‰ï¼šåœ¨æ ‡é¢˜æˆ–å°é¢æå‡ºé—®é¢˜ï¼Œåœ¨ç‚¹å‡»åçš„å†…å®¹ä¸­å›ç­”
- é®æŒ¡å…³é”®ä¿¡æ¯ï¼šç”¨é©¬èµ›å…‹ã€è´´çº¸ç­‰é®æŒ¡æ ‡é¢˜æˆ–å°é¢çš„å…³é”®éƒ¨åˆ†
- è¯åªè¯´ä¸€åŠï¼šç”¨çœç•¥å·ã€ä¸­æ–­ã€ç•™ç™½ç­‰æ–¹å¼æˆªæ–­å¥å­æˆ–æ•…äº‹
- è®¾ç½®æ‚¬å¿µï¼šä½¿ç”¨å„ç§å½¢å¼å¯¹ç¼ºå¤±çš„ä¿¡æ¯è¿›è¡Œé“ºå«ã€æ¸²æŸ“
- åªæŠ›å‡ºæƒ…å¢ƒï¼šä¾‹å¦‚â€œå½“...â€ã€â€œpovï¼šâ€¦â€
- æŒ‡ä»£ä¸æ˜ï¼šç”¨"è¿™ä¸ª""é‚£ä¸ª"æŒ‡ä»£ï¼Œä½†ä¸çŸ¥é“æŒ‡ä»£çš„åˆ°åº•æ˜¯ä»€ä¹ˆ
ï¼ˆæˆ–ï¼šä»¥ä¸Šæ²¡æœ‰åˆ—ä¸¾ä½†ç¬¦åˆæ ¸å¿ƒå®šä¹‰çš„çº¿ç´¢ï¼‰
æ³¨æ„ï¼šAlmost all social media cards have titles. Do NOT label 'Information Gap' just because there is a title.
Core defined boundaries (to avoid cognitive divergence)ï¼š
1.Exclusionary boundaries: Incomplete information caused by the limitations of preview section in terms of length and display format (such as the upper limit of title characters, cover image size) does not belong to information gaps;
2.Initiative boundaries: The gap is subjectively and deliberately designed by the creator, rather than being objectively restricted in content expression. The core is "could have finished but intentionally didn't";
3.Core boundaries: What is missing is the core information of the content (such as results, answers, key details, core conclusions), not insignificant auxiliary information.


hook4: Anomaly and novelty
æ ¸å¿ƒå®šä¹‰ï¼šå†…å®¹è¢«æ•…æ„åŒ…è£…æˆæƒŠäººã€è¿åå¸¸ç†æˆ–ç½•è§ã€æ–°å¥‡çš„ï¼Œä»¥æ¿€å‘å—ä¼—çš„å¥½å¥‡å¿ƒã€‚
æ“ä½œåŒ–åˆ¤æ–­ï¼šå†…å®¹ä¸­æ˜¯å¦åŒ…å«äº†ï¼šè¡¨ç°æƒŠäººåå¸¸çš„çŸ­è¯­ OR è¡¨ç°ç½•è§æ–°å¥‡çš„çŸ­è¯­
å‚ç…§çº¿ç´¢ï¼š
- è¡¨ç°æƒŠäººåå¸¸çš„çŸ­è¯­ï¼šç«Ÿç„¶/å±…ç„¶/æ²¡æƒ³åˆ°/ä¸å¯æ€è®®/ç½•è§/ç¬¬ä¸€æ¬¡è§/éœ‡æƒŠ/æƒŠå‘†/çœ‹å‚»/åˆ·æ–°ä¸‰è§‚/åˆ·æ–°è®¤çŸ¥/ä¸å¯æ€è®®/ç¥å¥‡â€¦
- è¡¨ç°ç½•è§æ–°å¥‡çš„çŸ­è¯­åŒ…æ‹¬ï¼š
  - ç›´æ¥å£°ç§°æ–°å¥‡çš„è¯ï¼šç‹¬åˆ›/åˆ«å…·ä¸€æ ¼/æ ‡æ–°ç«‹å¼‚/æ–°é¢–â€¦
  - æé™è¯ï¼šæœ€/é¡¶/è¶…/ç¬¬ä¸€/å²è¯—çº§â€¦
  - ç¨€ç¼ºæ€§ï¼šå”¯ä¸€/åªæœ‰/é™å®š/é²œè§/å¶å‘/å­¤ä¾‹å­¤å“/å°ä¼—/å†·é—¨/åƒå¹´ä¸€é‡â€¦
ï¼ˆæ³¨æ„ï¼šå¦‚æœå†…å®¹æœ¬èº«å¤Ÿåå¸¸æ–°å¥‡ä½†æ²¡æœ‰è¢«ä½œè€…åŒ…è£…ï¼Œåˆ™ä¸å±äºæ­¤ç±»å‹ï¼›æ­¤å¤–ï¼Œå¸¸è§„åˆ†äº«ã€æƒ…ç»ªå®£æ³„ä¸­åªæœ‰åŒ…å«äº†ä¸Šè¿°è¯æ±‡æˆ–ç±»ä¼¼è¯æ±‡çš„æ‰å±äºæ­¤ç±»å‹ï¼‰
Instruction: Do NOT label '1' just for What you consider novel, interesting, or contrary to common sense. What we are looking for is the action and intention of the author in packaging the content into a striking contrast or rare novelty.


hook5: Perceptual Contrast
æ ¸å¿ƒå®šä¹‰ï¼šå†…å®¹é€šè¿‡è§†è§‰æˆ–è€…æ–‡å­—å°†ä¸¤ç§æˆ–å¤šç§å½¢æˆåå·®æ„ä¹‰çš„çŠ¶æ€æˆ–äº‹ç‰©æ”¾åœ¨ä¸€èµ·ï¼Œä»¥æ¿€å‘å—ä¼—çš„æ¢ç´¢æ¬²ã€‚
æ“ä½œåŒ–åˆ¤æ–­ï¼šæœ‰æ²¡æœ‰ä¸¤ä¸ªåŠå¤šä¸ªæœ‰æ˜æ˜¾åå·®æ„ä¹‰çš„å¯¹æ¯”é¡¹ï¼Œæœ‰æ¯”è¾ƒä½†å·®åˆ«ä¸å¤§çš„ä¸ç®—
å‚ç…§çº¿ç´¢ï¼š
- å¯¹æ¯”é¡¹åŒ…æ‹¬ï¼šå‰å/å·¦å³/æ­£å/å¥½å/é¢„æœŸä¸ç°å®/åˆ«äººä¸è‡ªå·±â€¦ç­‰
- å¯¹æ¯”å½¢å¼åŒ…æ‹¬ï¼šæ–‡æœ¬å’Œæ–‡æœ¬çš„è¯­ä¹‰åå·®ã€å›¾åƒä¹‹é—´çš„è§†è§‰åå·®ã€æ–‡æœ¬ä¸å›¾åƒä¹‹é—´çš„åå·®


hook6: Ingroup Identification / Outgroup Distinction
æ ¸å¿ƒå®šä¹‰ï¼šå†…å®¹é€šè¿‡ç¾¤ä½“æ ‡ç­¾ï¼Œæ¿€å‘æŸä¸€ç¾¤ä½“å†…çš„è®¤åŒã€å½’å±ï¼›æˆ–æ¿€å‘å¯¹æŸä¸€ç¾¤ä½“çš„æ’æ–¥ã€è°ƒä¾ƒã€‚
æ“ä½œåŒ–åˆ¤æ–­ï¼šå†…å®¹ä¸­æ˜¯å¦ï¼šå‡ºç°ç¾¤ä½“æ ‡ç­¾ ANDï¼ˆå‡ºç°å½’å±/æ’æ–¥æ€åº¦ OR è¡ŒåŠ¨å¬å”¤ OR ç¾¤ä½“å…±æ€§ï¼‰
å‚ç…§çº¿ç´¢ï¼š
- ç¾¤ä½“æ ‡ç­¾ï¼šæåŠæŸä¸€ç¾¤ä½“ï¼Œå¦‚
  - æåˆ°ï¼šå’Œç§æ—/å›½å®¶/æ°‘æ—/å®—æ•™/åœ°åŸŸâ€¦ç­‰ç›¸å…³çš„åè¯
  - æåˆ°ï¼šå’Œå­¦æ ¡/å…¬å¸/ç»„ç»‡/æœºæ„/ç¤¾åŒºâ€¦ç­‰ç›¸å…³çš„åè¯
  - æåˆ°ï¼šå’Œå¹´é¾„/æ€§åˆ«/èŒä¸šâ€¦ç­‰ç›¸å…³çš„åè¯
  - ä»¥åŠå…¶ä»–ï¼šæŸç§çˆ±å¥½æ ‡ç­¾/æŸç§æ€§æ ¼æ ‡ç­¾/æŸç§æ˜Ÿåº§æ ‡ç­¾/æŸç§mbtiæ ‡ç­¾â€¦ç­‰
  ï¼ˆåªè¦å‡ºç°çš„çŸ­è¯­å¯ä»¥åœ¨äººç¾¤ä¸­åˆ’åˆ†å‡ºä¸€ä¸ªç¾¤ä½“å’Œå¦å¤–çš„äººï¼Œå³å¯ï¼‰
- å½’å±/æ’æ–¥æ€åº¦ï¼šè¡¨ç°å‡ºéª„å‚²ã€è‡ªè±ªã€è®¤åŒã€å…±æƒ…â€¦ï¼›æˆ–ï¼šé„™è§†ã€è°ƒä¾ƒã€è®½åˆºâ€¦ç­‰
- è¡ŒåŠ¨å¬å”¤ï¼šâ€œ...å¿…çœ‹â€ã€â€œæ˜¯...å°±ç‚¹èµâ€ã€â€œ...ä»¬è¡ŒåŠ¨èµ·æ¥â€â€¦ç­‰
- ç¾¤ä½“å…±æ€§ï¼šâ€œæ¯ä¸ª...éƒ½ç»å†è¿‡â€ã€â€œ...çš„æ—¥å¸¸â€ã€â€œ...éƒ½æ‡‚â€â€¦ä»¥åŠä½“ç°åœ¨ç”»é¢ä¸­çš„å…±æ€§ç‰¹å¾ç­‰
Instruction: Be SENSITIVE. If you suspect the content contains any jargon or visual style specific to a niche group, even if you are not 100% sure, Please also take it into consideration.


hook7: Social Comparison
æ ¸å¿ƒå®šä¹‰ï¼š å†…å®¹é€šè¿‡ç›´æ¥ä½¿ç”¨æ˜æ˜¾çš„æ¯”è¾ƒè¯ã€å±•ç¤ºå·®è·ã€æˆ–å±•ç¤ºç¤¾ä¼šæ¯”è¾ƒåçš„æŸç§æ€åº¦ç­‰ï¼Œæ¥å¼•å‘å—ä¼—å‚ä¸æ¯”è¾ƒã€‚
æ“ä½œåŒ–åˆ¤æ–­ï¼š
å†…å®¹ä¸­æ˜¯å¦ï¼šæ˜æ˜¾çš„æ¯”è¾ƒè¡Œä¸ºè¯ ORï¼ˆå±•ç¤ºå·®è· AND å±•ç¤ºäº†ç¤¾ä¼šæ¯”è¾ƒåæ€åº¦ï¼‰
å‚ç…§çº¿ç´¢ï¼š
- æ˜æ˜¾çš„æ¯”è¾ƒè¡Œä¸ºè¯ï¼šæ¯”.../æ›´/VS/ä¸å¦‚.../ç¢¾å‹/ç§’æ€â€¦ç­‰
- å·®è·ï¼šèƒ½åŠ›å·®è·ï¼ˆæŠ€èƒ½ã€æˆå°±ã€ä»»åŠ¡ç»©æ•ˆç­‰ï¼‰/ä¸ªäººç‰¹è´¨å·®è·ï¼ˆå¤–è²Œã€æ€§æ ¼ã€å¤©èµ‹ã€èº«é«˜ç­‰ï¼‰/èµ„æºå·®è·ï¼ˆè´¢å¯Œã€ç”Ÿæ´»æ°´å¹³ã€åœ°ä½é«˜ä½ã€æƒåˆ©å·®åˆ«ç­‰ï¼‰â€¦
- æ¯”è¾ƒåçš„æ€åº¦ï¼š -å‘ä¸Šæ¯”è¾ƒï¼šå«‰å¦’ã€è‡ªå‘ç­‰æ¶ˆææ€åº¦ï¼›è®¤å¯ã€æ¿€åŠ±ä¸Šè¿›ç­‰ç§¯ææ€åº¦
                                  -å‘ä¸‹æ¯”è¾ƒï¼šç‚«è€€ã€ä¼˜è¶Šæ„Ÿç­‰æ¶ˆææ€åº¦ï¼›çæƒœã€çŸ¥è¶³ç­‰ç§¯ææ€åº¦
æ³¨æ„ï¼š
1.å·®è·çš„å‚ç…§ç‚¹å¯ä»¥æ˜¯å†…å®¹ä¸­ç›´æ¥å±•ç°çš„å¯¹è±¡ï¼Œä¹Ÿå¯ä»¥æ˜¯å¤§ä¼—é»˜è®¤çš„æ™®éæ°´å¹³ï¼›å› æ­¤è¯·æ’é™¤å¸¸è§„çš„æ™®éæ°´å¹³çš„åˆ†äº«ç±»ç¬”è®°ï¼Œè¿™ä¸æ„æˆç¤¾ä¼šæ¯”è¾ƒã€‚
2.ç¤¾ä¼šæ¯”è¾ƒçš„æ¯”è¾ƒå¯¹è±¡æ˜¯è‡ªå·±ä¸åˆ«äººæ¯”è¾ƒã€åˆ«äººä¸åˆ«äººæ¯”è¾ƒã€ç¾¤ä½“ä¸ç¾¤ä½“æ¯”è¾ƒï¼ˆä¸åŒ…å«è‡ªå·±ä¸è‡ªå·±æ¯”è¾ƒã€å•çº¯çš„ç‰©å“æ¯”è¾ƒï¼‰


hook8: Authority Endorsement
æ ¸å¿ƒå®šä¹‰ï¼šæ–‡å­—æˆ–å›¾ç‰‡é€šè¿‡å„ç§æœ‰è¯´æœåŠ›çš„ä¿¡æºèƒŒä¹¦æ¥è±å…å—ä¼—çš„è´¨ç–‘æˆæœ¬ï¼Œä»è€Œå¼•å¯¼å—ä¼—ä¿¡æœæˆ–æ¨¡ä»¿ã€‚
æ“ä½œåŒ–åˆ¤æ–­ï¼šæ–‡å­—æˆ–å›¾ç‰‡ä¸­æ˜¯å¦å‡ºç°ï¼šä¿¡æºèƒŒä¹¦
å‚ç…§çº¿ç´¢ï¼š
- æƒå¨ä¿¡æºåŒ…æ‹¬ï¼šä¸“å®¶/æ•™æˆ/æœºæ„/åäºº/ç ”ç©¶/æ’å/è®¤è¯/å¥–é¡¹/æ•°å­—â€¦ç­‰
  - å¦‚ï¼šå“ˆä½›å¤§å­¦ç ”ç©¶/æ®...æŠ¥é“/...ä¸“å®¶è¯´/FDAè®¤è¯/æŸæŸæ˜æ˜ŸåŒæ¬¾/é’å²›ç¬¬ä¸€çš„/22wäººçœ‹è¿‡çš„â€¦
æ³¨æ„ï¼šå‡¡æ˜¯èƒ½å¢åŠ å¯ä¿¡åº¦çš„çº¿ç´¢éƒ½åœ¨èŒƒå›´å†…ï¼Œå…ˆæ ‡æ³¨èµ·æ¥æ¯”æ¼æ ‡è¦å¥½ã€‚

[Note] 
1. First, find reasons to label it as 1; only if no clues exist, label it as 0.
2. INDEPENDENT JUDGMENT: Judge the target post independently based on its own content.


[Thought Process required]
Before making a decision, you MUST:
1. Identify visual elements in the image (stickers, mosaics, layout).
2. Analyze the semantic intent of the title.
3. Combine them to consider the author's creative intent.
4. Compare them with the core definitions.


[Json Output Format]
Return the results in JSON format. Ensure 'reasoning' comes FIRST,If you output any text outside the JSON object (including analysis), it will be treated as invalid:
{
  "reasoning": "Step-by-step shot Chinese analysis of why hook1-8 is/isn't present (<40 chars)",
  "h1":0/1, "h2":0/1, "h3":0/1, "h4":0/1, "h5":0/1, "h6":0/1, "h7":0/1, "h8":0/1
}

"""

# ===================== 4. å·¥å…·å‡½æ•° =====================
def get_cos_client():
    config = CosConfig(Region=COS_CONFIG['Region'], SecretId=COS_CONFIG['SecretId'], SecretKey=COS_CONFIG['SecretKey'])
    return CosS3Client(config)

def load_title_map(label_title_csv: str) -> dict:
    df = pd.read_csv(label_title_csv, dtype=str, keep_default_na=False)
    df["post_id"] = df["post_id"].astype(str).str.strip().str.lower()
    df["title"] = df["title"].astype(str)
    return dict(zip(df["post_id"], df["title"]))

def fetch_cover_b64(cos_client, pid: str) -> str:
    key = f"downloads/{pid}/cover.jpg"
    obj = cos_client.get_object(Bucket=COS_CONFIG["Bucket"], Key=key)
    data = obj["Body"].get_raw_stream().read()
    return base64.b64encode(data).decode("utf-8")

def safe_parse_json(text: str):
    if not text:
        return None
    s = text.strip()

    s = re.sub(r"```json\s*", "", s, flags=re.IGNORECASE)
    s = s.replace("```", "")

    # ä»ç¬¬ä¸€ä¸ª { å¼€å§‹åšæ‹¬å·é…å¯¹ï¼Œæˆªå–ç¬¬ä¸€ä¸ªå®Œæ•´ JSON å¯¹è±¡
    start = s.find("{")
    if start == -1:
        return None

    depth = 0
    in_str = False
    esc = False
    for i in range(start, len(s)):
        ch = s[i]
        if in_str:
            if esc:
                esc = False
            elif ch == "\\":
                esc = True
            elif ch == '"':
                in_str = False
        else:
            if ch == '"':
                in_str = True
            elif ch == "{":
                depth += 1
            elif ch == "}":
                depth -= 1
                if depth == 0:
                    candidate = s[start:i+1]
                    try:
                        return json.loads(candidate)
                    except:
                        return None
    return None

def normalize_01(v):
    if isinstance(v, bool):
        return int(v)
    if isinstance(v, (int, float)):
        return int(v) if v in (0, 1) else None
    if isinstance(v, str):
        t = v.strip()
        if t == "0":
            return 0
        if t == "1":
            return 1
    return None

def validate_and_extract(raw_text: str):
    obj = safe_parse_json(raw_text)
    if not isinstance(obj, dict):
        return False, None, "JSON_PARSE_FAILED"

    for k in obj.keys():
        m = re.match(r"^(h|hook)(\d+)$", str(k).strip())
        if m and (int(m.group(2)) < 1 or int(m.group(2)) > 8):
            return False, None, f"EXTRA_HOOK_KEY_{k}"

    extracted = {"reasoning": str(obj.get("reasoning", "")).strip()}
    if not extracted["reasoning"]:
        return False, None, "EMPTY_REASONING"

    for i in range(1, 9):
        val = normalize_01(obj.get(f"h{i}"))
        if val is None:
            return False, None, f"INVALID_VALUE_h{i}"
        extracted[f"h{i}"] = val

    return True, extracted, ""

def _short(s: str, n: int):
    s = "" if s is None else str(s)
    s = s.replace("\n", "\\n")
    return s[:n] + ("..." if len(s) > n else "")

def is_rate_limited_exception(e: Exception) -> bool:
    msg = str(e).lower()
    keywords = [
        "rate limit", "ratelimit", "too many requests", "429",
        "quota", "throttl", "exceeded", "temporarily unavailable"
    ]
    if any(k in msg for k in keywords):
        return True

    if hasattr(e, "status_code") and getattr(e, "status_code", None) == 429:
        return True
    if hasattr(e, "response") and getattr(e, "response", None) is not None:
        try:
            if getattr(e.response, "status_code", None) == 429:
                return True
        except Exception:
            pass
    return False

def format_exception_detail(e: Exception) -> str:
    parts = [f"{type(e).__name__}: {e}"]
    if hasattr(e, "status_code") and getattr(e, "status_code", None):
        parts.append(f"status_code={getattr(e, 'status_code')}")
    if hasattr(e, "body") and getattr(e, "body", None):
        parts.append(f"body={_short(getattr(e, 'body'), 800)}")
    if hasattr(e, "response") and getattr(e, "response", None):
        try:
            r = getattr(e, "response")
            if hasattr(r, "status_code"):
                parts.append(f"resp_status={r.status_code}")
        except Exception:
            pass
    return " | ".join(parts)

# ===================== 4.1 å¢é‡å†™å…¥ CSV =====================
def load_done_post_ids(output_csv: Path) -> set:
    if (not output_csv.exists()) or output_csv.stat().st_size == 0:
        return set()
    try:
        df = pd.read_csv(output_csv, dtype=str, usecols=["post_id"], keep_default_na=False)
        return set(df["post_id"].astype(str).str.strip().str.lower().tolist())
    except Exception:
        return set()

def open_csv_appender(output_csv: Path, fieldnames: list):
    need_header = (not output_csv.exists()) or output_csv.stat().st_size == 0
    f = open(output_csv, "a", newline="", encoding="utf-8-sig")
    writer = csv.DictWriter(f, fieldnames=fieldnames)
    if need_header:
        writer.writeheader()
        f.flush()
    return f, writer

def write_one_row(writer, f_handle, rec: dict):
    writer.writerow(rec)
    f_handle.flush()


# ===================== 5. ä¸»æµç¨‹ =====================
def run_test():
    system_prompt = BASE_HOOK_DEFINITIONS

    df_task = pd.read_csv(TASK_CSV, dtype=str, keep_default_na=False)
    df_task["post_id"] = df_task["post_id"].astype(str).str.strip().str.lower()

    title_map = load_title_map(TITLE_LOOKUP_CSV)
    cos_client = get_cos_client()

    OUT_COLS = ["post_id"] + [f"h{i}" for i in range(1, 9)] + ["title", "reasoning"]
    output_path = RESULT_DIR / "predictions_qwen_1.csv"

    done = set()
    if RESUME_IF_EXISTS:
        done = load_done_post_ids(output_path)
        if done:
            print(f"ğŸ” RESUME: æ£€æµ‹åˆ°å·²å®Œæˆ {len(done)} æ¡ï¼Œå°†è‡ªåŠ¨è·³è¿‡ã€‚è¾“å‡ºï¼š{output_path}")

    f_out, writer = open_csv_appender(output_path, OUT_COLS)

    total_model_calls = 0
    invalid_model_calls = 0

    try:
        for _, row in tqdm(df_task.iterrows(), total=len(df_task), desc=f"Running {MODEL_NAME}"):
            pid = row["post_id"]
            if RESUME_IF_EXISTS and pid in done:
                continue

            title = title_map.get(pid, "")
            try:
                img_b64 = fetch_cover_b64(cos_client, pid)
            except Exception as e:
                if PRINT_EXCEPTION_DETAIL:
                    tqdm.write(f"[COS_ERROR] pid={pid} | {format_exception_detail(e)}")
                rec = {"post_id": pid, "title": title, "reasoning": "MISSING_COVER_OR_COS_ERROR", **{f"h{i}": 0 for i in range(1, 9)}}
                if WRITE_EVERY_ROW:
                    write_one_row(writer, f_out, rec)
                done.add(pid)
                if SLEEP_PER_ROW_SEC > 0:
                    time.sleep(SLEEP_PER_ROW_SEC)
                continue

            final = None
            last_err = ""

            # æ— æ•ˆè¾“å‡ºæœ€å¤š 3 æ¬¡
            invalid_attempts = 0

            # é™æµ/429å•ç‹¬é‡è¯•ï¼Œä¸æ¶ˆè€— invalid_attempts
            rate_retry = 0
            backoff = RATE_LIMIT_BASE_SLEEP

            while invalid_attempts < 3:
                total_model_calls += 1
                try:
                    resp = ai_client.chat.completions.create(
                        model=MODEL_NAME,
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": [
                                {"type": "text", "text": f"å¾…æ ‡æ³¨æ ‡é¢˜: {title}"},
                                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{img_b64}"}}
                            ]}
                        ],
                        temperature=0,
                    )

                    raw_content = resp.choices[0].message.content
                    ok, extracted, err = validate_and_extract(raw_content)

                    if ok:
                        final = extracted
                        break

                    # å†…å®¹æ— æ•ˆï¼šè®¡å…¥ invalid_attempts
                    invalid_model_calls += 1
                    invalid_attempts += 1
                    last_err = err

                    if PRINT_INVALID_DETAIL:
                        tqdm.write(f"[INVALID_OUTPUT] pid={pid} | invalid_try={invalid_attempts}/3 | err={err}")
                        if err == "JSON_PARSE_FAILED" and PRINT_RAW_PREVIEW_ON_PARSE_FAIL:
                            tqdm.write(f"[RAW_PREVIEW] pid={pid} | {_short(raw_content, RAW_PREVIEW_CHARS)}")

                    time.sleep(0.3)

                except Exception as e:
                    if is_rate_limited_exception(e):
                        rate_retry += 1
                        if PRINT_EXCEPTION_DETAIL:
                            tqdm.write(f"[RATE_LIMIT] pid={pid} | retry={rate_retry}/{RATE_LIMIT_MAX_RETRY} | {format_exception_detail(e)}")

                        if rate_retry > RATE_LIMIT_MAX_RETRY:
                            last_err = "RATE_LIMIT_MAX_RETRY_EXCEEDED"
                            break

                        sleep_s = min(RATE_LIMIT_MAX_SLEEP, backoff)
                        jitter = 1.0 + random.uniform(-RATE_LIMIT_JITTER, RATE_LIMIT_JITTER)
                        sleep_s = max(0.5, sleep_s * jitter)

                        tqdm.write(f"[RATE_LIMIT_SLEEP] pid={pid} | sleep={sleep_s:.1f}s")
                        time.sleep(sleep_s)
                        backoff *= 2.0
                        continue

                    invalid_model_calls += 1
                    invalid_attempts += 1
                    last_err = f"API_EXCEPTION_{type(e).__name__}"
                    if PRINT_EXCEPTION_DETAIL:
                        tqdm.write(f"[API_EXCEPTION] pid={pid} | invalid_try={invalid_attempts}/3 | {format_exception_detail(e)}")

                    time.sleep(0.8)

            if final is None:
                tqdm.write(f"[FINAL_FAIL] pid={pid} | last_err={last_err}")
                rec = {"post_id": pid, "title": title, "reasoning": f"FAILED:{last_err}", **{f"h{i}": 0 for i in range(1, 9)}}
            else:
                rec = {"post_id": pid, "title": title, "reasoning": final["reasoning"], **{f"h{i}": final[f"h{i}"] for i in range(1, 9)}}

            if WRITE_EVERY_ROW:
                write_one_row(writer, f_out, rec)

            done.add(pid)

            if SLEEP_PER_ROW_SEC > 0:
                time.sleep(SLEEP_PER_ROW_SEC)

    finally:
        f_out.close()

    invalid_rate = (invalid_model_calls / total_model_calls) if total_model_calls > 0 else 0.0
    stats = {
        "model_name": MODEL_NAME,
        "task_csv": TASK_CSV,
        "output_csv": str(output_path),
        "total_model_calls": total_model_calls,
        "invalid_model_calls": invalid_model_calls,
        "invalid_rate": f"{invalid_rate:.4%}",
        "processed_rows": len(done),
        "sleep_per_row_sec": SLEEP_PER_ROW_SEC,
        "rate_limit_base_sleep": RATE_LIMIT_BASE_SLEEP,
        "rate_limit_max_sleep": RATE_LIMIT_MAX_SLEEP,
        "rate_limit_max_retry": RATE_LIMIT_MAX_RETRY,
    }
    with open(RESULT_DIR / "stats.json", "w", encoding="utf-8") as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… å®Œæˆï¼è¾“å‡ºï¼š{output_path}")
    print(f"ğŸ“Š è°ƒç”¨æ€»æ•° {total_model_calls} | æ— æ•ˆè¾“å‡º {invalid_model_calls} | æ— æ•ˆç‡ {invalid_rate:.4%} | å·²å†™å…¥ {len(done)} è¡Œ")


if __name__ == "__main__":
    run_test()
